{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_duplicate_rows(df, num_rows, num_repeats):\n",
    "    # 从数据框中随机选择行索引\n",
    "    indices = np.random.choice(df.index, num_rows, replace=True)\n",
    "    # 重复选中的行\n",
    "    repeated_rows = df.loc[indices].sample(frac=1).reset_index(drop=True)\n",
    "    # 多次重复这些行\n",
    "    duplicates = pd.concat([repeated_rows] * num_repeats, ignore_index=True)\n",
    "    # 将重复的行附加到原始数据框中\n",
    "    df_extended = pd.concat([df, duplicates]).reset_index(drop=True)\n",
    "    return df_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# social_media_data = social_media_data.sample(100)\n",
    "# social_media_data = random_duplicate_rows(social_media_data, 10, 10)\n",
    "# social_media_data[\"IID\"] = np.random.randint(10000,20000,social_media_data.shape[0])\n",
    "# output_file = 'output.csv'\n",
    "# social_media_data.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载社交媒体数据\n",
    "social_media_data = pd.read_csv('output.csv')\n",
    "\n",
    "# 查看前几行数据\n",
    "social_media_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_length(data):\n",
    "    data['comment_length'] = data['SentimentText'].apply(len)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data['comment_length'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Comment Length')\n",
    "    plt.xlabel('Comment Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def lookF(data):\n",
    "        # 词袋模型\n",
    "    vectorizer = CountVectorizer(max_features=1000)\n",
    "    X_bow = vectorizer.fit_transform(data['SentimentText'])\n",
    "\n",
    "    # 可视化词频\n",
    "    # 获取词汇表和对应的词频\n",
    "    word_freq = dict(zip(vectorizer.get_feature_names_out(), X_bow.sum(axis=0).tolist()[0]))\n",
    "\n",
    "    # 将词频字典转换为DataFrame以便使用Seaborn绘图\n",
    "    word_freq_df = pd.DataFrame(list(word_freq.items()), columns=['word', 'frequency'])\n",
    "\n",
    "    # 按频率降序排序并选择前20个单词\n",
    "    top_words = word_freq_df.nlargest(20, 'frequency')\n",
    "\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='frequency', y='word', data=top_words)\n",
    "    plt.title('Top 20 Most Frequent Words')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_length(social_media_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_media_data.dropna(subset=['SentimentText'], inplace=True)\n",
    "new_data = social_media_data.copy()\n",
    "# social_media_data.drop_duplicates(subset=['SentimentText'], inplace=True)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "lookF(social_media_data)\n",
    "\n",
    "social_media_data['Text'] = social_media_data['SentimentText'].apply(clean_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X_bow = vectorizer.fit_transform(social_media_data['Text'])\n",
    "\n",
    "# 可视化词频\n",
    "# 获取词汇表和对应的词频\n",
    "word_freq = dict(zip(vectorizer.get_feature_names_out(), X_bow.sum(axis=0).tolist()[0]))\n",
    "\n",
    "# 将词频字典转换为DataFrame以便使用Seaborn绘图\n",
    "word_freq_df = pd.DataFrame(list(word_freq.items()), columns=['word', 'frequency'])\n",
    "\n",
    "# 按频率降序排序并选择前20个单词\n",
    "top_words = word_freq_df.nlargest(20, 'frequency')\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='frequency', y='word', data=top_words)\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 情感分析\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "social_media_data['sentiment'] = social_media_data['SentimentText'].apply(get_sentiment)\n",
    "social_media_data['comment_length'] = social_media_data['SentimentText'].apply(len)\n",
    "\n",
    "\n",
    "# 清洗前的词频\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(social_media_data['sentiment'], bins=50, kde=True)\n",
    "plt.title('Sentiment of Comments')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_emotion(text):\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=\"sk-qoVw2NSrjOFwER6d71E73a5c8984470494Dd9dD19d5d7bDe\",\n",
    "            base_url=\"https://api.bianxieai.com/v1\"\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Analyze the emotion of the following text:\\n\\n{text}\\n Make sure you only give emotion words, or None if you can't parse them\"}\n",
    "            ]\n",
    "        )\n",
    "        emotion_analysis = response.choices[0].message.content\n",
    "        return emotion_analysis\n",
    "    \n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "data = social_media_data['SentimentText'].head(10).apply(analyze_emotion)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = social_media_data.copy()\n",
    "new_data.drop_duplicates(subset=['SentimentText'], inplace=True)\n",
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codetr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
